1) openCV images coordinates are y,x or height, width
2) OpenCV 0,0 is upper left corner
3) In Translation Negative values move left and up, positive move right, and down.

Define a translation matrix to shift an image 30 pixels up and 50 pixels to the right.
M = np.float32([[1, 0, 50], [0, 1, -30]])

Define a translation matrix to shift an image 90 pixels down and 10 pixels to to the left.
M = np.float32([[1, 0, 50], [0, 1, 90]])

4) In Rotation Negative numbers is clockwise and positive numbers are counter clockwise
5) general rule, cv2.INTER_LINEAR  interpolation method is recommended as the default for whenever you’re upsampling or downsampling

6) when using flip 1 is horizontal, 0 is vertical, and -1 is both horizontal and vertical flipping.

7) Bitwise on is greater than 0, off = 0.
    AND: A bitwise AND is true if and only if both pixels are greater than zero.
    OR: A bitwise OR is true if either of the two pixels are greater than zero.
    XOR: A bitwise XOR is true if and only if one of the two pixels is greater than zero, but not both.
    NOT: A bitwise NOT inverts the “on” and “off” pixels in an image.
8) Essentially, the bitwise NOT function flips pixel values

9) Kernels can be an arbitrary size of M \times N pixels, provided that both M and N are odd integers.

10) as the size of the kernel increases, so will the amount in which the image is blurred.
11) always consider your lighting conditions before you write a single line of code!

12) We could also use the Scharr kernel instead of the Sobel kernel which may give us better approximations to the gradient

Note: While it’s not present in the code sample above, computing the Scharr kernel can be done in the exact same manner,
 only using the cv2.Scharr  function or supplying an optional parameter value of ksize=-1 for the cv2.Sobel  function
    # compute gradients along the X and Y axis, respectively
    gX = cv2.Sobel(gray, ddepth=cv2.CV_64F, dx=1, dy=0)
    gY = cv2.Sobel(gray, ddepth=cv2.CV_64F, dx=0, dy=1)
    
13) Contours are an invaluable tool as it's used to find and access our outlines.

All too often I see computer vision and image processing developers trying to leverage complicated machine learning techniques to solve problems where contours would be better suited.

Don’t fall into this trap — pay attention to this module, as it’s one of the most important techniques that you’ll use over and over again in your computer vision career.

14) Key takeaway: For better accuracy you’ll normally want to utilize a binary image rather than a grayscale image.
15)  The cv2.findContours  function is destructive to the input image (meaning that it manipulates it) so if you intend on using your input image again, be sure to clone it using the copy()  method prior to passing it into cv2.findContours .

16)  cv2.CHAIN_APPROX_NONE , we would be storing every single (x, y)-coordinate along the contour. In general, this not advisable. It’s substantially slower and takes up significantly more memory.
17) Returning only external contours is a technique that we’ll exploit heavily in this course, so it’s important for us to understand this concept.

18) It’s important to take note of Line 98. A contour must have at least 5 points for an ellipse to be computed — if a contour has less than 5 points, then an ellipse cannot be fit to the rotated rectangle region.

19) It’s important to note that we only apply connected-component analysis to binary or thresholded images.
20) Computing contour properties is done on only a single contour at a time




